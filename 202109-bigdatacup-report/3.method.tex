\section{Method} \label{sec:method}

% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width=\linewidth]{figures/net.png}
%     \caption{Overall structure of our method.}
%     \label{fig:net}
% \end{figure*}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/reweightloss.png}
    \caption{Session-Aware Loss Reweighting}
    \label{fig:reweightloss}
\end{figure*}

The overall structure of our method is shown in Figure \ref{fig:net}. 
In what follows, we will introduce each of our designs respectively.


%%%%%%%%%%%%%%%%%%%%%%%%%%
% two head
\subsection{Two-headed (Buy and Session) Prediction} \label{sec:method:twohead}
% \chen{1.to improve it, we can provide formulations rather than only texts; 2. to explain the motivations of each design}
We start with a two-headed prediction structure (Figure \ref{fig:net} left).
The network takes the following inputs: user profile features, user clicked items' id and features, 9 exposed target items' id and features.
%
These inputs are further processed by their corresponding embedding layers, and a backbone network (\textit{e.g.} MLP).
%
Finally, the network predicts whether the user will buy the 9 exposed target items or not, and to which session (\textit{c.f.} Section \ref{sec:eda:stats}) will the user unlock.
%
The predicted session will be used to refine the predicted results of the 9 exposed target items.

% design motivation
We design this framework for the following reasons:
%
Firstly, the 9 item feedbacks are correlated. For example, users might buy all of the first 6 items, only to buy the 9th item. It is not suitable to predict the 9 feedbacks independently. So we design a network that is able to predict 9 feedbacks simultaneously.
%
Secondly, if we only predict the 9 feedbacks, there might be mistakes where the predicted results are impossible to happen in the real world. For example, the network might predict one only buys the 1st and the 9th items. This is impossible since the user has to buy all of the first 6 items, in order to buy the 9th item. So we design another head to predict the unlocked sessions of users. The results are used to fix unreasonable outputs of the buy feedback predictions.



%%%%%%%%%%%%%%%%%%%%%%%%%%
% reweight
\subsection{Session-Aware Loss Reweighting}


To better model users' buy behaviors, we classify the 9 exposed items into 4 categories according to each user's buy behavior (weak positive, strong positive, strong negative, weak negative) as shown in Figure \ref{fig:reweightloss}.
%
For sessions before the last session user has unlocked, items should be treated as weak positives, as user might buy these items only to unlock the later sessions.
%
For the last session user has unlocked, items should be treated as strong positives and strong negatives. As user unlocked and stopped in this session, items bought or not bought should be classified as strong signals.
%
For later locked sessions, items should be treated as weak negatives, as users haven't unlocked these sessions, we shouldn't assume too strong preference on these items.

In practice, we reweight the BCE loss of these 4 categories using weight 0.5, 1, 1, 0.5, respectively.
In our experiments, reweighting loss provide huge improvements to the final score (from 0.365 to 0.388).


%%%%%%%%%%%%%%%%%%%%%%%%%%
% multitask click
\subsection{Multi-tasking with Click Behavior Prediction}
Apart from the network described in \ref{sec:method:twohead}, we use another network to predict user click behavior (Figure \ref{fig:net} right).
The two networks share the same embedding layers.
We argue that through this multi-tasking procedure, the embedding layers can be better learned.
%
In our early ablation study, this can improve our score from 0.362 to 0.365.


%%%%%%%%%%%%%%%%%%%%%%%%%%
% transformer
\subsection{Transformer Backbone}
Instead of simple MLPs \cite{din, ncf}, we switch the backbone part of buy behavior prediction and click behavior prediction into transformers \cite{transformer}, as their self-attention mechanism is proved to be effective on capturing inter-relations between different features.
%
In our early ablation study, this can improve our score from 0.36 to 0.365.




%%%%%%%%%%%%%%%%%%%%%%%%%%
% aug, tta
\subsection{Randomness-in-session Augmentation}
To prevent over-fitting and make training more robust, we randomly shuffle items' order within the same sessions. 
This strategy is also used for test time augmentation, where orginal prediction and shuffled predictions are averaged to produce the final results.
In our experiments, augmentation in training provides minor difference, while augmentation in testing provides an unstable improvement of around 0.002.



%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Other settings}
Colab with a P100 GPU is used as our training platform.
We use Adam \cite{adam} with default hyper-parameters in PyTorch \cite {pytorch}.
Batch size is set to 32, and learning rate is set to 1e-2 for 10 epochs, and 1e-3 for another 10 epochs.
%
Clicking data in the test set of both tracks is also used during our training. 
%
All continuous features are discretized into bins (including item features, prices).


